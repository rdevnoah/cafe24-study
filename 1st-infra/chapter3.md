## chapter#3 OS 캐시와 분산 <sub>대규모 데이터를 효율척으로 처리하는 원리</sub>

- 앞선 내용에서 디스크와 메모리 간 속도차는 10<sup>5</sup>~10<sup>6</sup> 이상 난다고 하였다. 
- OS에는 디스크 내의 데이터에 빠르게 액세스 할 수 있는 구조가 있다.
  - OS 는 메모리를 이용하여 디스크 액세스를 줄인다.
  - 이 원리를 잘 이용하면 OS에 애플리케이션의 상당 부분을 맡김으로 부하를 줄일 수 있다. 이것이 바로 OS 캐시이다.
- 가상 메모리 구조란 무엇일까?
  - 메모리의 양은 정해져 있지만 사용자는 메모리의 용량을 넘는 프로그램을 충분히 PC에서 구동시킬 수 있다. 메모리는 용량이 부족하게 되면 가상 메모리 영역을 이용하게 되며 이 공간은 디스크의 공간을 활용한다. 그러므로 1GB의 메모리를 가진 PC에서도 포토샵이나 일러스트레이터와 같은 무거운 프로그램을 돌릴 수 있다(디스크를 사용하기 때문에 느리지만 말이다.). 
  - 하지만 프로세스는 구동되기 위해 결국 주소값을 알아야하고, 가상 메모리의 주소는 실제 물리 메모리가 가진 주소 일수도 있고 아닐수도 있다. 결국 이는 물레 어드레스와 선형 어드레스와 같은 개념을 통해 프로세스가 주소값을 가지게 되는 것이다. 물리 어드레스와 선형 어드레서의 경우 따로 학습하도록 한다. 
- Linux
  - **페이지 캐시(page cache)**나 파일 캐시(file cache)[^1], 버퍼 캐시(buffer cache)라는 캐시 구조를 가지고 있다.





#### Linux 페이징 구조란?

- 논리적인 선형 어드레스를 페이징 구조를 통해 물리 어드레스로 변환한다. 
- 결국 OS는 프로세스가 필요로 하는 (가상)메모리의 주소를 페이지를 통해 특정 크기로 나누어 저장하고 있고, 물리 메모리 또한 이 가상 메모리의 페이지와 같은 크기로 나누어 (프레임) 관리한다. 그리고 이 페이지와 프레임은 서로 매핑되어 있게 된다. 만일 가상 메모리 상에 아직 매핑되지 않은 물리 메모리의 프레임이 있고, 그 가상 메모리를 사용하려고 한다면 물리 메모리와 매핑되어야 하기 때문에 OS는 어떠한 방식으로든(LRU : 빈도수를 따지는 것) 기존 물리 메모리를 해제하고 그 공간을 가상메모리의 페이지와 매핑 시킨다(결국 프로세스는 물리 메모리의 어드레스를 할당받게 된다).



#### 페이지 캐싱이란?

- 메모리상에 한번 할당되어 사용된 페이지를 없애버리는 것이 아니라, 다른 프로세스가 다시 같은 어드레스의 데이터가 필요로 하게 되었을 때, 그 페이지를 그대로 줌으로써 다시 같은 작업을 반복할 필요가 없게 된다.(이미 한번 그 업무를 수행하여 결과값을 알고 있으므로)

- 가상 메모리의 최소 단위 : 페이지!

  - 하나의 큰 파일을 읽어야 할 때도, 결국 그 파일의 일부 영역만 캐싱 하능하게 한다.

  - 이는 OS가 파일의 데이터를 읽을 때, offset 과 같이 파일의 영역을 노드 번호로 할당해서 그 영역부터 읽을 수 있도록 하기 때문이다. 간단히 예를 든다면 자바에서 stream을 통해 데이터를 읽거나 쓸 때, 파라메터 값의 offset의 필요이유를 생각하면 좋을 것 같다. (아래의 코드에서 int off)

    ~~~java
    public abstract class InputStream{
    
    	...
    
    	abstract int read();
      
    	// 입력스트림으로부터 1byte를 읽어서 반환한다. 읽을 수 없으면 -1을 반환한다.
      
    	int read(byte[] b, int off, int len){
    
      // 입력스트림으로부터 ien개의 byte를 읽어서 byte배열 b의 off위치부터 저장한다.
    
    		...
    
    		for(int i-off, i < off+len; i++){			
    
    			b[i]=(byte)read();
    
    // read()를 호출해서 데이터를 읽어서 배열을 채운다.
    
    		}
    
    		...
    
    	}
    ~~~

    



#### VFS

- 리눅스의 파일 시스템 방법 
  - VFS는 메모리에 적재된 프로세스가 접근하는 디스크영역의 다양한 디바이스와, OS 사이에 존재하여, 더욱 추상화를 강화함으로써, 어떤 파일 시스템이 오더라도 동일한 방식으로 OS가 처리할 수 있도록 하게 해준다. 결국 이 VFS에 페이지 캐시 구조가 있음으로 인해 다양한 파일시스템이 동일하게 캐싱됨으로써 속도를 향상시키게 된다.

- 결국 시스템이 부팅된 후, 메모리는 디스크의 내용을 지속적으로 캐싱하고 있다. 프로세스가 메모리를 필요로 하고, 캐싱으로 인해 메모리에 공간이 없다면 LRU와 같은 기법으로 가장 적게 사용된 캐싱된 메모리를 프로세스에게 내어준다. 그리고 프로세스는 그곳을 사용함으로써 여러 디바이스에 접근하여 메모리에 페이지를 남기고, 그 페이지는 여타 다른 캐싱된 페이지와 같은 방법으로 지속적으로 관리된다.





### I/O 부하를 줄이는 방법

- 데이터 규모에 비해 메모리의 크기가 크면 부하를 줄일 수 있다. 이는 데이터의 크기를 압축함으로써 크기를 줄여 메모리의 사용을 효율적으로 하는 방법이 된다.
- 경제적인 비용을 계산하여 메모리를 관리하는 것. 결과적으로 큰 메모리를 사면 I/O의 부하를 줄이게 되는 것이지만, 이는 결국 큰 비용이 발생한다.

- 복수의 서버로 확장시키는 방법 또한 존재한다. 이 경우 ap 서버(WAS)를 늘리는 것과 DB서버를 늘리는것, 두 가지가 있는데, I/O의 경우 DB서버를 늘리는 것과 관련되어 있다. ap 서버를 늘리는 것은 cpu의 부하를 막을 수 있고, DB서버를 늘리는 것은 메모리를 늘리는 것(캐시 용량을 늘리는 것)이므로 I/O의 부하를 줄일 수 있다.
  - 하지만 이렇게 서버를 확장하는 것은 필요한 리소스가 많이 다르다. 또한 단순히 대수만 늘리는 것으로는 서버를 운용하는 방식에 따라 효과를 볼 수 없을 수도 있다. (예 : DB서버를 한대 더 증설하고 만약 운용 방식이 데이터의 안정화를 위해 복사하여 둔다는 형식이면, 결국 캐싱하는 용량의 비율은 같으므로)



### 국소성을 살리는 분산

- 국소성을 살리는 분산이란, DB서버 내의 수많은 데이터 중 접촉하는 액세스 패턴에 따라 데이터를 따로 관리한다는 것이다.
  - 예를 들면, 일반 사용자가 DB내의 데이터 중 어떤 데이터는 접촉하는 빈도수가 높고, 그렇지 않은 데이터도 있다고 가정하자. 이 때, 빈도수가 높은 데이터와 그렇지 않은 데이터를 서로 다른 DB서버로 요청을 보냄으로써, 접촉하고 싶어하는 데이터의 양 만큼만 DB서버의 메모리가 캐싱하기 편하도록 해주는 것이다. 
  - **액세스 패턴에 따라**, 보고싶어하는 데이터로의 요청이 있을 시에는 그 DB서버로, 그렇지 않을 경우는 반대의 DB서버로 돌려주면 그만이다. (하지만 이럴 경우, 보고싶어하는 데이터를 적당히 선정하여 하나의 메모리가 충분히 감당할 수 있도록 하는데 큰 의의가 있을 것으로 판단된다.) 
- 이러한 국소성을 살리는 분산에 다음과 같은 패턴들이 존재한다.
  - 파티셔닝
  - 요청 패턴을 '섬'으로 분할

#### 파티셔닝

- 파티셔닝은 한 대였던 DB서버를 여러 대로 분할하는 것을 의미한다. 간단하게는 테이블 단위로 분할하는 방법이 있다. 
  - (많은 요청이 있는 테이블 + 그렇지 않은 테이블)…(많은 요청이 있는 테이블 + 그렇지 않은 테이블) 을 적절히 분석하여 한 메모리가 감당할 수 있는 기준으로 파티셔닝을 한다면 각각 DB서버가 자신의 데이터를 모두 캐싱할 수 있으므로, 빠라진다. 결국 이는 사용자의 DB접속 로그를 확인하여 (다른 파티셔닝 패턴에서도 마찬가지로)적절히 나누는 것이 중요할 것이다.

- 다른 파티셔닝 기법은 무엇이 있을까?



#### 요청 패턴을 '섬'으로 분할

- 용도별로 시스템을 섬으로 나누는 것이다. 
- 일반적인 요청의 경우 프록시 서버에서 그 요청에 맞는 AP서버로 요청을 보내고 AP서버는 DB서버 아래로 내려간다. 
  - 예를 들면 일반 사용자의 요청의 경우 1번 서버로, 웹 봇의 경우 2번 서버로 보내 관리하는 것을 말한다. 
  - 이는 앞서 설명한 파티셔닝과 조금 헷갈릴 수도 있는데, 조금 쉽게 얘기하자면, 파티셔닝의 경우 일반 사용자들의 DB 요청 패턴을 파악하여 나누는 것이라면, 이것은 일반 사용자들보다 더 큰 영역에서 나누는 것이라고 이해하면 편할 것 같다. 결국 파티셔닝과 이 기법을 적절히 활용한다면 (운용 단계에서 분석과 해석해야할 자료들이 더 생기겠지만) 더 큰 효율을 발휘할 수 있을 것이다.



> 페이지 캐시를 고려한 운용의 기본 규칙
>
> - OS 기동 후에 곧바로 서버를 투입하지 않는다.
> - 성능평가는 캐시가 최적화 되었을 때
>
> 3챕터의 강의 포인트
>
> - 분석은 국소성을 고려해서 실시
>
> - 데이터 규모에 맞게 탑재 메모리를 조정한다.
>
>   -> 메모리 증설로 대응할 수 없다면 분산한다.



#### 부하분산과 OS동작 원리의 상관관계

- 지금까지 학습을 통해, OS의 동작원리를 이해하는것이 부하 분산과 어떤 상관관계가 있는지 알게 되었다.

  (예를 들어 캐시와 같은)OS의 동작원리를 모른체, 단지 부하를 위해 분산한다고 했다면 단순히 여러대로 호스트를 늘리는 것과 같이 추상적으로 이해했을 것이다. 

- OS의 동작원리를 배우면, 다양한 장치가 하드웨어를 효율적으로 사용하기 위해 어떤 원리를 갖추고 있는지를 비롯해 그 장점과 단점을 파악할 수 있다. 이러한 점을 알게 되면 시스템 전체를 최적화하는데 큰 도움이 된다. 이는 아래의 인용문을 통해 그 느낌이 와닿을 것이라 생각한다.

  > Apache나 MySQL 등의 미들웨어 사용버도 알고 있어야 하지만 이 부분은 어디까지나 How-to지 기초지식은 아니다. 배워야 할 것은 애초에 해당 미들웨어가 작동하고 있는 OS에 있다는 것이 개인적인 견해로, 이 책에서도 중점적으로 다루었다.





_ _ _

[^1]: OS는 파일 단위로 캐싱하는 것이 아니라 특정한 용량 만큼의 블록 단위로 캐싱하기 때문에, 파일캐싱은 옳지 않은 말이다.